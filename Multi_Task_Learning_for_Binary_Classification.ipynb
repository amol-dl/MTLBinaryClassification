{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi Task Learning for Binary Classification.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO62VfaKFQN1y/1D9E/6DgE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6I6b_tQDpFj",
        "colab_type": "text"
      },
      "source": [
        "### Mounting Drive and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAE1qauDDoA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT_FvguFDbZS",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msf_2JSADT6d",
        "colab_type": "code",
        "outputId": "67ebaff8-135d-429e-c902-3d3aae9c7adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "!pip install -q tensorflow-hub\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSuLNFllwAve",
        "colab_type": "text"
      },
      "source": [
        "### Loading IMDb Review Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kks-N50rqd7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for outfile in open('/content/drive/My Drive/IMDB_reviews.json','r'):\n",
        "  data.append(json.loads(outfile))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro0hWhr0GOkf",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTW3CnDuGXXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SplitData(id):\n",
        "  D1_text = []\n",
        "  D1_spoiler = []\n",
        "  D2_text = []\n",
        "  D2_spoiler = []\n",
        "  for d in data:\n",
        "    if d['movie_id'] == id:\n",
        "      D2_text.append(d['review_text'])\n",
        "      D2_spoiler.append(d['is_spoiler'])\n",
        "    else:\n",
        "      D1_text.append(d['review_text'])\n",
        "      D1_spoiler.append(d['is_spoiler'])\n",
        "  X1_train, X1_test, y1_train, y1_test = train_test_split(D1_text, D1_spoiler, test_size = 0.15, random_state = 20)\n",
        "  X2_train, X2_test, y2_train, y2_test = train_test_split(D2_text, D2_spoiler, test_size = 0.15, random_state = 20)\n",
        "  y1_train = pd.get_dummies(np.asarray(y1_train).astype(int))\n",
        "  y1_test = pd.get_dummies(np.asarray(y1_test).astype(int))\n",
        "  y2_train = pd.get_dummies(np.asarray(y2_train).astype(int))\n",
        "  y2_test = pd.get_dummies(np.asarray(y2_test).astype(int))\n",
        "  return X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GipTzlNuKNie",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6udGsOfKtlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def X2Model(l):\n",
        "  embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "  hub_layer = hub.KerasLayer(embedding, input_shape = [], output_shape = [128], dtype=tf.string, trainable = True)\n",
        "  if l < 100:\n",
        "    X2_Input = Input(shape=(), dtype=tf.string)\n",
        "    X2_HubLayer = hub_layer(X2_Input)\n",
        "    X2_Dense1 = Dense(16, activation='relu')(X2_HubLayer)\n",
        "    X2_Output = Dense(2, activation='softmax')(X2_Dense1)\n",
        "  elif l < 1000:\n",
        "    X2_Input = Input(shape=(), dtype=tf.string)\n",
        "    X2_HubLayer = hub_layer(X2_Input)\n",
        "    X2_Dense1 = Dense(36, activation='relu')(X2_HubLayer)\n",
        "    X2_Dense2 = Dense(12, activation='relu')(X2_Dense1)\n",
        "    X2_Output = Dense(2, activation='softmax')(X2_Dense2)\n",
        "  else:\n",
        "    X2_Input = Input(shape=(), dtype=tf.string)\n",
        "    X2_HubLayer = hub_layer(X2_Input)\n",
        "    X2_Dense1 = Dense(48, activation='relu')(X2_HubLayer)\n",
        "    X2_Dense2 = Dense(20, activation='relu')(X2_Dense1)\n",
        "    X2_Dense3 = Dense(8, activation='relu')(X2_Dense2)\n",
        "    X2_Output = Dense(2, activation='softmax')(X2_Dense3)\n",
        "  return X2_Input, X2_Output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAMEECnXKWMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BuildModel():\n",
        "  embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "  hub_layer = hub.KerasLayer(embedding, input_shape = [], output_shape = [128], dtype=tf.string, trainable = True)\n",
        "  X1_Input = Input(shape=(), dtype=tf.string)\n",
        "  X1_HubLayer = hub_layer(X1_Input)\n",
        "  X1_Dense1 = Dense(48, activation='relu')(X1_HubLayer)\n",
        "  X1_Dense2 = Dense(20, activation='relu')(X1_Dense1)\n",
        "  X1_Dense3 = Dense(8, activation='relu')(X1_Dense2)\n",
        "  X1_Output = Dense(2, activation='softmax')(X1_Dense3)\n",
        "  X2_Input, X2_Output = X2Model(len(X2_train))\n",
        "  model = Model([X1_Input,X2_Input],[X1_Output,X2_Output])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', loss_weights=[1, 0.01*float(len(X2_train_initial)/len(X1_train))], metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEdf3DP1vM18",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nXVjhH36lAGW",
        "colab": {}
      },
      "source": [
        "def TrainModel(model):\n",
        "  epoch = 1\n",
        "  batch_size = 1000\n",
        "  sum1 = 0\n",
        "  sum2 = 0\n",
        "  for i in range(int(len(X1_train)/batch_size)):\n",
        "    model.train_on_batch([np.asarray(X1_train[batch_size*i:batch_size*i + batch_size]),np.asarray(X2_train[batch_size*i:batch_size*i + batch_size])],\n",
        "                         [y1_train[batch_size*i:batch_size*i + batch_size],y2_train[batch_size*i:batch_size*i + batch_size]])\n",
        "  for i in range(int(len(X1_train)/batch_size)):\n",
        "    result = model.evaluate([np.asarray(X1_train[batch_size*i:batch_size*i + batch_size]),np.asarray(X2_train[batch_size*i:batch_size*i + batch_size])],\n",
        "                            [y1_train[batch_size*i:batch_size*i + batch_size],y2_train[batch_size*i:batch_size*i + batch_size]], verbose = 1)\n",
        "    sum1 = sum1 + result[3]      \n",
        "    sum2 = sum2 + result[4]\n",
        "  testresult = model.evaluate([np.asarray(X1_test),np.asarray(X2_test)],[y1_test,y2_test], verbose = 1)\n",
        "  print(\"Large Train Accuracy: %0.3f Large Test Accuracy: %0.3f Small Train Accuracy: %0.3f Small Test Accuracy: %0.3f\" % (sum1/int(len(X1_train)/batch_size),sum2/int(len(X2_train)/batch_size),testresult[3],testresult[4]))\n",
        "  return testresult[3], testresult[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRqd3y0IkAiT",
        "colab_type": "text"
      },
      "source": [
        "###Running the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKgRwD5LkKza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_list = []\n",
        "count = 0\n",
        "sum1 = 0\n",
        "sum2 = 0\n",
        "for d in data:\n",
        "  id_list.append(d['movie_id'])\n",
        "id_list = np.unique(id_list)\n",
        "for i in random.choices(id_list, k=10):\n",
        "  count = count + 1\n",
        "  X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test = SplitData(i)\n",
        "  X2_train_initial = X2_train\n",
        "  X2_train, y2_train = resample(X2_train, y2_train, n_samples = len(X1_train))\n",
        "  X2_test, y2_test = resample(X2_test, y2_test, n_samples = len(X1_test))\n",
        "  print(\"Movie Number : %3d Movie ID : %s Percent Complete : %0.2f\" % (count, i, count/len(id_list)))\n",
        "  model = BuildModel()\n",
        "  accuracy1, accuracy2 = TrainModel(model)\n",
        "  sum1 = sum1 + accuracy1\n",
        "  sum2 = sum2 + accuracy2\n",
        "print(\"Final Large Dataset Accuracy : %0.3f Small Dataset Accuracy : %0.3f\" % (sum1/len(id_list),sum2/len(id_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}